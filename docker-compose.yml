version: "3.9"

services:
  llama_cpp:
    build:
      context: .
      dockerfile: ./llama_cpp/Dockerfile
    ports:
      - 3001:3001
    volumes:
      - ./llama_cpp:/llama_cpp
    command: sh -c "python main.py"
  frontend:
    build:
      context: .
      dockerfile: ./frontend/Dockerfile
    ports:
      - 3000:3000
    volumes:
      - ./frontend:/frontend
      - ./frontend/node_modules:/frontend/node_modules
    command: sh -c "npm run dev"
